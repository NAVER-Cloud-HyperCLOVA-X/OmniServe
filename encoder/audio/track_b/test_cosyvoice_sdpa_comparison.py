#!/usr/bin/env python3
# Omni Chainer - Multimodal LLM Inference System
# Copyright (c) 2025-present NAVER Cloud Corp.
# Apache-2.0

"""
SDPA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ MultiHeadAttention ê²°ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸
"""
import torch
import sys
import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.model.cosyvoice import MultiHeadAttention, AudioEncoderV2


def create_mask(batch_size, seq_len, device):
    """Attention mask ìƒì„± (padding mask)"""
    # ì˜ˆì‹œ: ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •
    lengths = torch.tensor([seq_len] * batch_size, device=device)
    if batch_size >= 2:
        lengths[1] = max(1, seq_len - 2)
    if batch_size >= 3:
        lengths[2] = max(1, seq_len - 5)
    
    max_len = seq_len
    seq_range = torch.arange(0, max_len, dtype=torch.int64, device=device)
    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)
    seq_length_expand = lengths.unsqueeze(-1)
    mask = seq_range_expand >= seq_length_expand
    mask = ~mask  # True for non-padded, False for padded
    
    # bias maskë¡œ ë³€í™˜ (False -> -1e10, True -> 0)
    # mask shape: (B, T) -> (B, 1, T) -> (B, 1, 1, T) for broadcasting with (B, n_head, T, T)
    mask_bias = mask.to(torch.float32)
    mask_bias = (1.0 - mask_bias) * -1.0e10
    return mask_bias.unsqueeze(1).unsqueeze(1)  # (B, 1, 1, T)


def test_qkv_attention_comparison():
    """qkv_attention ë©”ì„œë“œ ì§ì ‘ ë¹„êµ"""
    print("=" * 80)
    print("qkv_attention ë©”ì„œë“œ ì§ì ‘ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
    batch_size = 2
    seq_len = 10
    n_state = 1280
    n_head = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"Device: {device}")
    print(f"Batch size: {batch_size}, Seq len: {seq_len}")
    print(f"n_state: {n_state}, n_head: {n_head}")
    print()
    
    # ë™ì¼í•œ ì…ë ¥ ìƒì„±
    torch.manual_seed(42)
    q = torch.randn(batch_size, seq_len, n_state, device=device)
    k = torch.randn(batch_size, seq_len, n_state, device=device)
    v = torch.randn(batch_size, seq_len, n_state, device=device)
    mask = create_mask(batch_size, seq_len, device)
    
    # ë‘ ê°œì˜ attention ëª¨ë“ˆ ìƒì„± (ê°€ì¤‘ì¹˜ ê³µìœ )
    attn_no_sdpa = MultiHeadAttention(n_state, n_head, use_sdpa=False).to(device)
    attn_sdpa = MultiHeadAttention(n_state, n_head, use_sdpa=True).to(device)
    
    # ê°€ì¤‘ì¹˜ ë³µì‚¬ (ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
    attn_sdpa.load_state_dict(attn_no_sdpa.state_dict())
    
    # qkv_attention ì§ì ‘ í˜¸ì¶œ
    print("1. use_sdpa=False ê²½ë¡œ ì‹¤í–‰...")
    with torch.no_grad():
        output_no_sdpa, qk_no_sdpa = attn_no_sdpa.qkv_attention(q, k, v, mask)
    
    print("2. use_sdpa=True ê²½ë¡œ ì‹¤í–‰...")
    with torch.no_grad():
        output_sdpa, qk_sdpa = attn_sdpa.qkv_attention(q, k, v, mask)
    
    # ê²°ê³¼ ë¹„êµ
    print("\n" + "-" * 80)
    print("ê²°ê³¼ ë¹„êµ:")
    print("-" * 80)
    print(f"Output shape (no_sdpa): {output_no_sdpa.shape}")
    print(f"Output shape (sdpa):    {output_sdpa.shape}")
    print()
    
    # ìˆ˜ì¹˜ ë¹„êµ
    max_diff = (output_no_sdpa - output_sdpa).abs().max().item()
    mean_diff = (output_no_sdpa - output_sdpa).abs().mean().item()
    rel_diff = ((output_no_sdpa - output_sdpa).abs() / (output_no_sdpa.abs() + 1e-8)).mean().item()
    
    print(f"Max absolute difference:  {max_diff:.2e}")
    print(f"Mean absolute difference: {mean_diff:.2e}")
    print(f"Mean relative difference: {rel_diff:.2e}")
    print()
    
    # ê±°ì˜ ë™ì¼í•œì§€ í™•ì¸ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ê³ ë ¤)
    tolerance = 1e-5
    is_close = torch.allclose(output_no_sdpa, output_sdpa, atol=tolerance, rtol=tolerance)
    print(f"Results are close (tol={tolerance}): {is_close}")
    
    if not is_close:
        print("\nâš ï¸  ê²½ê³ : ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
        # ì°¨ì´ê°€ í° ìœ„ì¹˜ ì°¾ê¸°
        diff = (output_no_sdpa - output_sdpa).abs()
        max_idx = diff.argmax()
        print(f"ìµœëŒ€ ì°¨ì´ ìœ„ì¹˜: {max_idx}")
        print(f"no_sdpa ê°’: {output_no_sdpa.flatten()[max_idx].item():.6f}")
        print(f"sdpa ê°’:    {output_sdpa.flatten()[max_idx].item():.6f}")
    else:
        print("\nâœ… ê²°ê³¼ê°€ ë™ì¼í•©ë‹ˆë‹¤!")
    
    return is_close


def test_full_attention_comparison():
    """ì „ì²´ forward ë©”ì„œë“œ ë¹„êµ"""
    print("\n" + "=" * 80)
    print("ì „ì²´ forward ë©”ì„œë“œ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
    batch_size = 2
    seq_len = 10
    n_state = 1280
    n_head = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"Device: {device}")
    print(f"Batch size: {batch_size}, Seq len: {seq_len}")
    print(f"n_state: {n_state}, n_head: {n_head}")
    print()
    
    # ë™ì¼í•œ ì…ë ¥ ìƒì„±
    torch.manual_seed(42)
    x = torch.randn(batch_size, seq_len, n_state, device=device)
    mask = create_mask(batch_size, seq_len, device)
    
    # ë‘ ê°œì˜ attention ëª¨ë“ˆ ìƒì„± (ê°€ì¤‘ì¹˜ ê³µìœ )
    attn_no_sdpa = MultiHeadAttention(n_state, n_head, use_sdpa=False).to(device)
    attn_sdpa = MultiHeadAttention(n_state, n_head, use_sdpa=True).to(device)
    
    # ê°€ì¤‘ì¹˜ ë³µì‚¬
    attn_sdpa.load_state_dict(attn_no_sdpa.state_dict())
    
    # Forward pass
    print("1. use_sdpa=False ê²½ë¡œ ì‹¤í–‰...")
    with torch.no_grad():
        output_no_sdpa, qk_no_sdpa = attn_no_sdpa(x, mask)
    
    print("2. use_sdpa=True ê²½ë¡œ ì‹¤í–‰...")
    with torch.no_grad():
        output_sdpa, qk_sdpa = attn_sdpa(x, mask)
    
    # ê²°ê³¼ ë¹„êµ
    print("\n" + "-" * 80)
    print("ê²°ê³¼ ë¹„êµ:")
    print("-" * 80)
    print(f"Output shape (no_sdpa): {output_no_sdpa.shape}")
    print(f"Output shape (sdpa):    {output_sdpa.shape}")
    print()
    
    max_diff = (output_no_sdpa - output_sdpa).abs().max().item()
    mean_diff = (output_no_sdpa - output_sdpa).abs().mean().item()
    rel_diff = ((output_no_sdpa - output_sdpa).abs() / (output_no_sdpa.abs() + 1e-8)).mean().item()
    
    print(f"Max absolute difference:  {max_diff:.2e}")
    print(f"Mean absolute difference: {mean_diff:.2e}")
    print(f"Mean relative difference: {rel_diff:.2e}")
    print()
    
    tolerance = 1e-5
    is_close = torch.allclose(output_no_sdpa, output_sdpa, atol=tolerance, rtol=tolerance)
    print(f"Results are close (tol={tolerance}): {is_close}")
    
    if not is_close:
        print("\nâš ï¸  ê²½ê³ : ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
    else:
        print("\nâœ… ê²°ê³¼ê°€ ë™ì¼í•©ë‹ˆë‹¤!")
    
    return is_close


def test_scale_verification():
    """Scale ì ìš© ë°©ì‹ ê²€ì¦"""
    print("\n" + "=" * 80)
    print("Scale ì ìš© ë°©ì‹ ê²€ì¦")
    print("=" * 80)
    
    batch_size = 1
    seq_len = 5
    n_state = 1280
    n_head = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    D = n_state
    scale = (D // n_head) ** -0.25
    print(f"Scale value: {scale:.6f}")
    print(f"Scale^2:     {scale**2:.6f}")
    print()
    
    # ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ìˆ˜ë™ ê³„ì‚°
    torch.manual_seed(42)
    q = torch.randn(batch_size, seq_len, n_state, device=device)
    k = torch.randn(batch_size, seq_len, n_state, device=device)
    v = torch.randn(batch_size, seq_len, n_state, device=device)
    
    # Reshape
    q_reshaped = q.view(batch_size, seq_len, n_head, -1)
    k_reshaped = k.view(batch_size, seq_len, n_head, -1)
    v_reshaped = v.view(batch_size, seq_len, n_head, -1)
    
    # no_sdpa ë°©ì‹
    q_no_sdpa = q_reshaped.permute(0, 2, 1, 3) * scale  # (B, n_head, T, D//n_head)
    k_no_sdpa = k_reshaped.permute(0, 2, 3, 1) * scale  # (B, n_head, D//n_head, T)
    qk_no_sdpa = q_no_sdpa @ k_no_sdpa  # (B, n_head, T, T)
    
    # sdpa ë°©ì‹
    q_sdpa = q_reshaped.permute(0, 2, 1, 3) * scale  # (B, n_head, T, D//n_head)
    k_sdpa = k_reshaped.permute(0, 2, 1, 3) * scale  # (B, n_head, T, D//n_head)
    # SDPA ë‚´ë¶€: q @ k.transpose(-2, -1)
    qk_sdpa_manual = q_sdpa @ k_sdpa.transpose(-2, -1)  # (B, n_head, T, T)
    
    print("qk ê³„ì‚° ë¹„êµ:")
    print(f"no_sdpa qk shape: {qk_no_sdpa.shape}")
    print(f"sdpa qk shape:    {qk_sdpa_manual.shape}")
    
    max_diff_qk = (qk_no_sdpa - qk_sdpa_manual).abs().max().item()
    print(f"qk max difference: {max_diff_qk:.2e}")
    
    if max_diff_qk < 1e-5:
        print("âœ… qk ê³„ì‚°ì´ ë™ì¼í•©ë‹ˆë‹¤!")
    else:
        print("âš ï¸  qk ê³„ì‚°ì´ ë‹¤ë¦…ë‹ˆë‹¤!")


if __name__ == "__main__":
    print("SDPA vs Non-SDPA ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # Scale ê²€ì¦
        test_scale_verification()
        
        # qkv_attention ì§ì ‘ ë¹„êµ
        result1 = test_qkv_attention_comparison()
        
        # ì „ì²´ forward ë¹„êµ
        result2 = test_full_attention_comparison()
        
        print("\n" + "=" * 80)
        print("ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 80)
        print(f"qkv_attention ë¹„êµ: {'âœ… í†µê³¼' if result1 else 'âŒ ì‹¤íŒ¨'}")
        print(f"forward ë¹„êµ:       {'âœ… í†µê³¼' if result2 else 'âŒ ì‹¤íŒ¨'}")
        
        if result1 and result2:
            print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ë¡œì§ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

