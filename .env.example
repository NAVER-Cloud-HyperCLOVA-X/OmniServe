# ============================================
# OMNI-CHAINER CONFIGURATION
# ============================================
# Copy this file to .env and fill in your values
# cp .env.example .env

# ============================================
# SERVICE ENDPOINTS (Internal Docker Network)
# ============================================
# These are the internal endpoints used by omni-chainer
# Usually no need to change unless you modify docker-compose.yml

# ---- Track A Endpoints ----
TRACK_A_LLM_ENDPOINT=http://localhost:10021/v1/chat/completions
TRACK_A_VISION_ENCODING_ENDPOINT=http://localhost:10004/process_image_or_video

# ---- Track B Endpoints ----
TRACK_B_LLM_ENDPOINT=http://localhost:10032/v1/chat/completions
TRACK_B_VISION_ENCODING_ENDPOINT=http://localhost:10064/process_image_or_video
TRACK_B_VISION_DECODING_ENDPOINT=http://localhost:10063/decode
TRACK_B_AUDIO_ENCODING_ENDPOINT=http://localhost:10002/process_audio
TRACK_B_AUDIO_DECODING_ENDPOINT=http://localhost:11180/predictions

# ============================================
# S3/OBJECT STORAGE SETTINGS
# ============================================
# Required for storing generated images and audio files
# Supports AWS S3 or any S3-compatible storage (MinIO, NCP Object Storage, etc.)

NCP_S3_SERVICE=s3
NCP_S3_ENDPOINT=https://your-s3-endpoint.com
NCP_S3_REGION=your-region
NCP_S3_ACCESS_KEY=your-access-key
NCP_S3_SECRET_KEY=your-secret-key
NCP_S3_BUCKET_NAME=your-bucket-name

# Alias for compatibility (used by encoders/decoders)
WBL_S3_BUCKET_NAME=${NCP_S3_BUCKET_NAME}

# ============================================
# MODEL PATHS
# ============================================
# Set these to the locations where you downloaded/converted the models
# Use convert_model.py to extract components from unified HuggingFace models

# Model IDs (used for internal identification)
VLM_ENCODER_VISION_MODEL_ID=track_a_vision_encoder
OMNI_ENCODER_VISION_MODEL_ID=track_b_vision_encoder
OMNI_ENCODER_AUDIO_MODEL_ID=track_b_audio_encoder

# ---- Track A Models ----
# Download from: https://huggingface.co/NaverCloud/HyperCLOVAX-SEED-Think-32B
# Then convert: python convert_model.py --input /path/to/model --output ./track_a --track a
VLM_MODEL_PATH=/path/to/track_a/llm/HyperCLOVAX-SEED-Think-32B
VLM_ENCODER_VISION_MODEL_PATH=/path/to/track_a/ve/HyperCLOVAX-SEED-Think-32B

# ---- Track B Models ----
# Download from: https://huggingface.co/NaverCloud/HyperCLOVAX-SEED-Omni-8B
# Then convert: python convert_model.py --input /path/to/model --output ./track_b --track b
OMNI_MODEL_PATH=/path/to/track_b/llm/HyperCLOVAX-SEED-Omni-8B
OMNI_ENCODER_AUDIO_MODEL_PATH=/path/to/track_b/ae/HyperCLOVAX-SEED-Omni-8B
OMNI_ENCODER_VISION_MODEL_PATH=/path/to/track_b/ve/HyperCLOVAX-SEED-Omni-8B
OMNI_DECODER_VISION_MODEL_PATH=/path/to/track_b/vd/HyperCLOVAX-SEED-Omni-8B
OMNI_DECODER_AUDIO_TORCHSERVE_MODEL_PATH=/path/to/track_b/ad/HyperCLOVAX-SEED-Omni-8B

# ============================================
# SERVICE PORTS (Host Machine)
# ============================================
# These ports will be exposed on your host machine
# Adjust if you have port conflicts

OMNI_CHAINER_API_PORT=8000
VLM_LLM_PORT=10021
VLM_ENCODER_VISION_API_PORT=10004
OMNI_PORT=10032
OMNI_ENCODER_AUDIO_API_PORT=10002
OMNI_ENCODER_VISION_API_PORT=10064
OMNI_DECODER_VISION_API_PORT=10063
OMNI_DECODER_AUDIO_API_PORT=11180
OMNI_DECODER_AUDIO_TORCHSERVE_PORT=11181
